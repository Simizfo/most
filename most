#!/usr/bin/python3

import sqlite3
import os
import requests
import sys
from bs4 import BeautifulSoup

if len(sys.argv) < 2:
    print('''Error: MOST requires an url as starting point. See
    --help or -h for more.''')
    exit()

if sys.argv[1] == '--help' or sys.argv[1] == '-h':
    print('''
    MOST v0.1 - created by Simone Franco
    @simizfo on Twitter - me@simonefranco.net
    A basic crawler for learning purposes.

    Usage ---
    main.py url
    
    where url is the starting point of the crawler. This will generate
    a sqlite3 db containing a table with the crawled urls, each having
    its reference and the round when it is scanned.
    ''')
    exit()

print(sys.argv[1])

def getURL(page):
    """

    :param page: html of web page (here: Python home page) 
    :return: urls in that page 
    """
    start_link = page.find("a href")
    if start_link == -1:
        return None, 0
    start_quote = page.find('"', start_link)
    end_quote = page.find('"', start_quote + 1)
    url = page[start_quote + 1: end_quote]
    return url, end_quote

if(os.path.exists('data/data.db')):
    os.remove('data/data.db')

if(os.path.exists('data')):
    os.rmdir('data')

if not os.path.exists('data'):
    os.mkdir('data')

sqlconnection = sqlite3.connect('data/data.db')

sql = sqlconnection.cursor()

sql.execute('CREATE TABLE IF NOT EXISTS urls (url text, ref text, round int);')

sql.execute('INSERT INTO urls VALUES (?, null, 0);', (sys.argv[1],))

sqlconnection.commit()

round = 0

while True:
    nextround_count = sql.execute('SELECT count(*) FROM urls WHERE round = ?', (round,))
    print('MOST is ready. Round ' + str(round) + '. I will scan ' + str(nextround_count.fetchone()[0]) + ' urls. Press y to confirm and go.')

    choice = input()
    if choice == 'y':
        query = sql.execute('SELECT * FROM urls WHERE round = ?', (round,))
        results = query.fetchall()
        print(len(results))
        for row in results:
            print(row)
            try:
                response = requests.get(row[0])
            except:
                print('Maybe not a valid url? check more!')
                continue
            page = str(BeautifulSoup(response.content, features="html.parser"))
            url = 'starting'
            while url:
                url, n = getURL(page)
                page = page[n:]
                if url:
                    if url.startswith('/'):
                        url = row[0] + url
                    print(url)
                    sql.execute('INSERT INTO urls VALUES (?, ?, ?)', (url, row[0], round + 1))

        sqlconnection.commit()
        round += 1
    
    else: 
        break
